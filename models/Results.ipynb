{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of the seismic interpretation\n",
    "\n",
    "This notebooks sums up our current progress in multiple seismic-related tasks:\n",
    "\n",
    "* [Horizon detection](detection)\n",
    "* [Horizon extension](extension)\n",
    "* [Interlayers segmentation](segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='detection'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is closely connected with day-to-day work of seismic experts: the very first thing they need to work with the cube is to pick seismic horizons. Seismic horizon is a change in rock properties across a boundary between two layers of rock, particularly seismic velocity and density. Such changes are visible in seismic images (even for an untrained eye), and could be automatically detected. We have a [notebook](./Horizons_detection.ipynb) that demonstrates entire process of solving the task: from indexing dataset and data-feeding procedure to training neural network and computing the metrics. Some of the most important parts:\n",
    "\n",
    "* **Dataset:** we have 4 seismic cubes, each with 4-10 hand-labeled horizons. The smallest one is held out for testing purposes\n",
    "\n",
    "* **Data-feeding procedure:** due to the fact that we usually can't feed entire cube into GPU (only the smallest ones), we cut them into crops and use it as training data. We also use some augmentations like scaling, cutout, additive noise, elastic transform etc\n",
    "\n",
    "* **Neural network architecture:** `EncoderDecoder` is used; we use sophisticated building blocks (like `Inception` ones) while reducing spatial resolution in order to obtain information about inner structure of the crop, and then restore initial resolution to produce fine-grained labels\n",
    "\n",
    "* **Training procedure:** we use `Adam` optimizer for 1000 epochs to minimize `Dice` coefficient; 80% of ilines in each of the three cubes are used at train time\n",
    "\n",
    "* **Validation:** we first validate our model on the remaining 20% of the ilines in the three cubes; then we check model performance on the held-out cube in order to fairly evaluate model quality\n",
    "\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "We use multiple metrics: area of detected horizon (compared to the hand-labeled one), mean difference between horizons, area of detected horizon that is closer than 5ms to the ground truth. In order to provide exhaustive research, we also trained multiple models on different datasets.\n",
    "\n",
    "First of all, we created individual models for each of the three cubes. Moreover, we used only each 200-th iline, totalling in no more than 15 slides. Due to that fact that seismic cubes change slowly along its spatial dimensions, we expect our model to easily recover the rest of horizons. For conveniency, we created `Python` [scripts](./../scripts), that essentially repeat described pipeline but easier to mass-usage. Predictions from each model, tested on the same cube, scored following results:\n",
    "\n",
    "| Train/test cube |   Area, % | Mean error, ms | Area in 5ms window, % |\n",
    "| :------ | ----: | ----: | ----: |\n",
    "| CUBE_1 | 90, 91, 86  | 1.5, 2.1, 4.4 | 96, 94, 85 |\n",
    "| CUBE_2 | 100, 98, 95 | 2.4, 2.1, 2.8 | 96, 99.9, 96.4 |\n",
    "| CUBE_3 | 92, 92, 92  | 1.8, 1.9, 2.5 | 99.8, 99.5, 91.6 | \n",
    "| CUBE_4 |  73, 83, 84 | 3, 3.6, 4.3 | 89, 86, 84 |\n",
    "\n",
    "Results in the table are consistent with our observations: detected horizonts are detected well, and the main point of improvement lies in enlarging the covered area. The only thing that catches the eye is suspiciously low area of detected horizons on `Cube_1`. Returning to the dataset [description](./../datasets/Horizonts_modelling.ipynb), we can easily identify the roots of the problem: hand-labeled horizons are for some reasons present in the zero-traces, and that skews our results.\n",
    "\n",
    "Models, trained only on one cube, hardly works on the others: it just can'd adjust to the altering of values in the traces. Thus, we need to train model on at least multiple cubes in order to generalize on the others. \n",
    "\n",
    "Having 3 cubes in total, we trained models for each pair of them and used it as predictor on the remaining one: results (with the same metrics as before) are shown in the table:\n",
    "\n",
    "| NOT in the training |   Area, % | Mean error, ms | Area in 5ms window, % |\n",
    "| :------ | ----: | ----: | ----: |\n",
    "| CUBE_1 | 5  | 0.85 | 94 |\n",
    "| CUBE_3 | 45  | 1 | 97 | \n",
    "| CUBE_4 |  25 | 50 | 0 |\n",
    "\n",
    "Again, results are not surprising: where test-cube structure resembles the train ones, model can follow the horizon quite well, with the easiest cube being labeled the best. It is the covered area that is the problem: different types of inner noises does not allow model to generalize well enough on big distances. The hardest cube (`Cube_4`) stays pretty much unlabeled due to its unique hardness.\n",
    "\n",
    "### Criticism\n",
    "\n",
    "The task in hand is ill-defined: it is very unclear, which horizons on the slide we want to get, how many of them, with which rules of picking. Current labels are quite inconsistent: they have different phase (some of them are on maximum values of amplitude, some of them at minimums); they separate different objects: most of them are following the brightest line on the slide, some of them are between crucial seismic facies, few of them track fissures. Most of them are made by automatic autocorellation and not really interesting (nor hard).\n",
    "\n",
    "Despite that, this model serves as a great trampoline for the others: [horizont extension](./Horizons_extension.ipynb) and [facies segmentation](./Segmenting_interlayers.ipynb). Clear visual interpretation allows to compare different neural network architectures and test all of the methods of our library.\n",
    "\n",
    "### Suggestions for improvements\n",
    "\n",
    "It is easily seen that in this task the bottleneck is data: more diverse seismic cubes with more horizons would greatly improve both quality of the detected ones and allow to better generalize on completely unseen data. Nevertheless, we can use this type of models even in its current state to generate more data for other tasks: namely, [horizon extension](./Horizons_extension.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extension'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was mentioned above, the task of horizon detection is ill-defined due to the fact that it is unclear which horizons we want to get when testing the model on a new cube. That is why we came up with another formulation of the problem - [horizon extension](./Horizons_extension.ipynb).\n",
    "\n",
    "In a real-life scenario, a seismic specialist wants to *segment a specific horizon* and in order to solve this task, prediction-time, we provide our model with prior information about location of a small segment of the target horizon.\n",
    "\n",
    "We have the same **Dataset** and **Data feeding procedure** as in Horizon detection model with a diffence that we train model on crops of shape (2, 64, 64) and model input consists of a concatenation of seismic crop and prior mask where only small part of the true mask remained non-zero. During training on crops we created prior mask using the real mask.\n",
    "\n",
    "\n",
    "* **Neural network architecture:** [Tiramisu architecture](https://arxiv.org/abs/1611.09326) is used that is EncoderDecoder architecture based on DenseNet blocks.\n",
    "\n",
    "* **Training procedure:** we use `Adam` optimizer for 2000 epochs to minimize `Dice` coefficient; 80% of ilines in each of two cubes are used at train time\n",
    "\n",
    "* **Validation:** we first validate our model crops from the remaining 20% of the ilines in the two cubes; then we check model performance on the held-out cube in order to fairly evaluate model quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a horizon on a whole slice or cube we created a function that sequentially runs predict on a grid of overlapping crops so that each subsequent crop can use predicted mask from the previous crop as a prior mask needed for the model setup.\n",
    "\n",
    "Although model shows satisfactory results on crops, whole area prediction task it to be improved.\n",
    "In simple cases like Cube_3 where horizons are easy to track and the horizon extension procedure works quite well but in more complicated cases with faults, sault domes and noisy traces current extension procedure fails to track the horizon labeled by an expert.\n",
    "\n",
    "Here is table of metrics (area of detected horizon that is closer than 5ms to the ground truth) for some plain horizons on different cubes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Train/test cube |  Length | Test on same cube, % | Test on new cube, % |\n",
    "| :------ | ----: | ----: | ----: |\n",
    "| CUBE_3 | 844  | 100 | 92.1 |\n",
    "| CUBE_1 | 974  | 39.9 | 39.9 | \n",
    "| CUBE_4 |  1157 | 59.5  | 55 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='segmentation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a significant decrease in metrics on Cube_1 and Cube_4 that have a more complicated geologic structure than Cube_3. It happens mostly due to the error sensitivity of the extension procedure that is performed subsequently on overlapping crops. If once we make a mistake and miss the true phase we will hardly get back to the initial horizon.\n",
    "\n",
    "The horizon extension procedure makes a subsequent predict on one crop at a time and takes more than 2 hours to track a horizon on a whole Cube_3. It can be sufficiently sped up by running prediction for different slices in parallel.\n",
    "\n",
    "Also, it is crucial to improve model performance on non-trivial horizons, for example by providing additional information about the horizon to the model to reduce the chance of changing phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interlayers segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
